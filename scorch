#!/usr/bin/env python3

# ISC License (ISC)
#
# Copyright (c) 2016, Antonio SJ Musumeci <trapexit@spawn.link>
#
# Permission to use, copy, modify, and/or distribute this software for any
# purpose with or without fee is hereby granted, provided that the above
# copyright notice and this permission notice appear in all copies.
#
# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

import argparse
import collections
import csv
import errno
import hashlib
import math
import os
import pickle
import random
import re
import shlex
import stat
import sys
import tempfile
import time
from itertools import takewhile


DEFAULT_DB = '/var/tmp/scorch.db'


class FileInfo(object):
    md5   = ''
    size  = 0
    mode  = 0
    mtime = 0

    def __init__(self,size,mode,mtime):
        self.size  = size
        self.mode  = mode
        self.mtime = mtime

    def __init__(self,md5,size,mode,mtime):
        self.md5   = md5
        self.size  = size
        self.mode  = mode
        self.mtime = mtime

    def __str__(self):
        return str({'md5': self.md5,
                    'size': self.size,
                    'mode': self.mode,
                    'mtime': self.mtime})


class FileFilter(object):
    def __init__(self,basepath,fnfilter,fifilter):
        self.basepath = basepath
        self.fnfilter = fnfilter
        self.fifilter = fifilter


    def filter(self,filepath,fi,other=(lambda f: False)):
        common = commonprefix([self.basepath,filepath])
        if common != self.basepath:
            return True
        if self.fnfilter(filepath):
            return True
        if self.fifilter(fi):
            return True
        if other(filepath):
            return True
        return False


def allnamesequal(name):
    return all(n==name[0] for n in name[1:])


def commonprefix(paths, sep='/'):
    bydirectorylevels = zip(*[p.split(sep) for p in paths])
    return sep.join(x[0] for x in takewhile(allnamesequal, bydirectorylevels))


def regex_type(pat):
    try:
        re.compile(pat)
    except:
        raise argparse.ArgumentTypeError
    return pat


def build_arg_parser():
    desc = 'a tool to help discover file corruption'
    parser = argparse.ArgumentParser(description=desc)
    parser.add_argument('-d','--db',
                        type=str,
                        default=DEFAULT_DB,
                        help='database which stores hashes')
    parser.add_argument('inst',
                        choices=['add','append','check',
                                 'check+update','delete',
                                 'cleanup','list',
                                 'list-unhashed','list-dups',
                                 'list-solo','list-missing','find'],
                        help='actions')
    parser.add_argument('dir',
                        type=str,
                        nargs='+',
                        help='directories to work on')
    parser.add_argument('-v','--verbose',
                        action='count',
                        default=0,
                        help='print details of files')
    parser.add_argument('-r','--restrict',
                        choices=['sticky',
                                 'readonly'],
                        help='restrict action to certain types of files')
    parser.add_argument('-f','--fnfilter',
                        type=regex_type,
                        help='restrict action to files which match regex')
    parser.add_argument('-s','--sort',
                        choices=['none','radix','reverse-radix',
                                 'natural','reverse-natural','random',
                                 'time-asc','time-desc'],
                        help='when adding/appending/checking sort files before acting on them')
    parser.add_argument('-m','--max',
                        type=int,
                        default=sys.maxsize,
                        help='max number of actions to take')
    parser.add_argument('-b','--break-on-error',
                        action='store_true',
                        default=False,
                        help='break on first failure / error')

    return parser


def hash_file(filepath, hasher=None, blocksize=65536):
    if not hasher:
        hasher = hashlib.md5()

    with open(filepath,'rb') as afile:
        buf = afile.read(blocksize)
        while buf:
            hasher.update(buf)
            buf = afile.read(blocksize)

    return hasher.hexdigest()


def get_files(basepath,filefilter,db={}):
    if os.path.isfile(basepath):
        fi = get_fileinfo(basepath)
        if not fi:
            return []
        return [(basepath,fi)]

    filelist = []
    for (dirname,dirnames,filenames) in os.walk(basepath):
        for filename in filenames:
            filepath = os.path.join(dirname,filename)
            if filepath in db:
                continue

            fi = get_fileinfo(filepath)
            if not fi:
                continue

            if filefilter.filter(filepath,fi):
                continue

            filelist.append((filepath,fi))

    return filelist


def filter_files(files,filefilter,other=(lambda f: False)):
    if filefilter.basepath in files:
        return files

    rv = []
    for (filepath,fi) in files:
        if filefilter.filter(filepath,fi,other):
            continue
        rv.append((filepath,fi))

    return rv


def get_fileinfo(filepath):
    try:
        st = os.lstat(filepath)
        if not stat.S_ISREG(st.st_mode):
            return None
        return FileInfo(md5='',
                        size=st.st_size,
                        mode=st.st_mode,
                        mtime=st.st_mtime)
    except:
        return None


def print_filepath(count,total,filepath):
    padding = len(str(total))
    padded = str(count).zfill(padding)
    s = '{0}/{1} {2}: '.format(padded,total,filepath)
    print(s,end='')
    sys.stdout.flush()


def humansize(nbytes):
    suffixes = ['B','KB','MB','GB','TB','PB','ZB']
    rank = int(math.log(nbytes,1024)) if nbytes else 0
    rank = min(rank, len(suffixes) - 1)
    human = nbytes / (1024.0 ** rank)
    f = ('%.2f' % human).rstrip('0').rstrip('.')
    return '%s%s' % (f, suffixes[rank])


# Can't use inode since filesystems based on FUSE can have those change
# mount to mount
def different_files(oldfi,newfi):
    return ((oldfi.size  != newfi.size) and
            (oldfi.mtime != newfi.mtime))


def add_hashes(db,basepath,filefilter,sort,
               maxactions,verbose,breakonerror,
               dbadd,dbremove):
    rv = 1
    filepaths = get_files(basepath,filefilter)
    sort(filepaths)

    actions = 0
    total = min(maxactions,len(filepaths))
    for (filepath,fi) in filepaths:
        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        if verbose:
            print_filepath(actions,total,filepath)

        try:
            fi.md5 = hash_file(filepath)
            dbadd[filepath] = fi
            if verbose:
                print(fi.md5)
        except Exception as e:
            if verbose:
                print("ERROR:",e)
            if breakonerror:
                break

    return rv


def append_hashes(db,basepath,filefilter,sort,
                  maxactions,verbose,breakonerror,
                  dbadd,dbremove):
    rv = 1
    filepaths = get_files(basepath,filefilter,db)
    sort(filepaths)

    actions = 0
    total = min(maxactions,len(filepaths))
    for (filepath,fi) in filepaths:
        if actions >= maxactions:
            break
        actions += 1

        rv = 0
        if verbose:
            print_filepath(actions,total,filepath)

        try:
            fi.md5 = hash_file(filepath)
            dbadd[filepath] = fi
            if verbose:
                print(fi.md5)
        except (KeyboardInterrupt,SystemExit):
            raise
        except Exception as e:
            if verbose:
                print(e)
            if breakonerror:
                break

    return rv


def check_hashes(db,basepath,filefilter,sort,
                 maxactions,verbose,breakonerror,
                 dbadd,dbremove,update=False):
    rv = 0
    filepaths = filter_files(db.items(),filefilter)
    sort(filepaths)

    actions = 0
    total = min(maxactions,len(filepaths))
    for (filepath,oldfi) in filepaths:
        if actions >= maxactions:
            return rv
        actions += 1

        if verbose:
            print_filepath(actions,total,filepath)

        try:
            newfi = get_fileinfo(filepath)
            if different_files(oldfi,newfi):
                if not verbose:
                    print_filepath(actions,total,filepath)
                old_size = humansize(oldfi.size)
                new_size = humansize(newfi.size)
                old_time = time.ctime(oldfi.mtime)
                new_time = time.ctime(newfi.mtime)
                print("FILE CHANGED\n",
                      "- size:",old_size,"->",new_size,"\n"
                      " - mtime:",old_time,"->",new_time)
                if update:
                    newfi.md5 = hash_file(filepath)
                    dbadd[filepath] = newfi
                    print(" - updated hash:",newfi.md5)
            else:
                oldhashval = oldfi.md5
                newhashval = hash_file(filepath)
                if newhashval != oldhashval:
                    rv = 1
                    if not verbose:
                        print_filepath(actions,total,filepath)
                    print("FAILED")
                    if breakonerror:
                        break
                elif verbose:
                    print("OK")
        except (KeyboardInterrupt,SystemExit):
            raise
        except Exception as e:
            traceback.print_exc()
            if not verbose:
                print_filepath(actions,total,filepath)
            print('ERROR:',e)
            if breakonerror:
                break

    return rv


def check_and_update_hashes(db,basepath,filefilter,sort,
                            maxactions,verbose,breakonerror,
                            dbadd,dbremove):
    return check_hashes(db,basepath,filefilter,sort,
                        maxactions,verbose,breakonerror,
                        dbadd,dbremove,update=True)


def delete_hashes(db,basepath,filefilter,sort,
                  maxactions,verbose,breakonerror,
                  dbadd,dbremove):
    rv = 1
    filepaths = filter_files(db.items(),filefilter)
    sort(filepaths)

    actions = 0
    total = min(maxactions,len(filepaths))
    for (filepath,fi) in filepaths:
        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        dbremove.append(filepath)
        if verbose:
            print_filepath(actions,total,filepath)
            print("removed")

    return rv


def cleanup_hashes(db,basepath,filefilter,sort,
                   maxactions,verbose,breakonerror,
                   dbadd,dbremove):
    rv = 1
    filepaths = filter_files(db.items(),filefilter,os.path.exists)
    sort(filepaths)

    actions = 0
    total = min(maxactions,len(filepaths))
    for (filepath,fi) in filepaths:
        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        dbremove.append(filepath)
        if verbose:
            print_filepath(actions,total,filepath)
            print("removed")

    return rv


def list_hashes(db,basepath,filefilter,sort,
                maxactions,verbose,breakonerror,
                dbadd,dbremove):
    rv = 1
    filepaths = filter_files(db.items(),filefilter)
    sort(filepaths)

    actions = 0
    for (filepath,fi) in filepaths:
        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        if verbose:
            print("{} {}".format(fi.md5,filepath))
        else:
            shortfilepath = filepath[len(basepath)+1:]
            shortfilepath = os.path.join('.',shortfilepath)
            print("{} {}".format(fi.md5,shortfilepath))

    return rv


def list_unhashed(db,basepath,filefilter,sort,
                  maxactions,verbose,breakonerror,
                  dbadd,dbremove):
    rv = 1
    filepaths = get_files(basepath,filefilter,db)
    sort(filepaths)

    actions = 0
    for (filepath,fi) in filepaths:
        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        print(filepath)

    return rv


def list_dups(db,basepath,filefilter,sort,
              maxactions,verbose,breakonerror,
              dbadd,dbremove):
    rv = 1
    hashdb = {}
    for (filepath,fi) in db.items():
        if filefilter.filter(filepath,fi):
            continue

        if not fi.md5 in hashdb:
            hashdb[fi.md5] = []

        hashdb[fi.md5].append(filepath)

    actions = 0
    for (hashval,filepaths) in hashdb.items():
        if len(filepaths) <= 1:
            continue

        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        filepaths = [shlex.quote(filepath) for filepath in filepaths]
        filepaths.sort()
        if verbose:
            print(hashval,'\n -','\n - '.join(filepaths))
        else:
            print(hashval,' '.join(filepaths))

    return rv


def list_solo(db,basepath,filefilter,sort,
              maxactions,verbose,breakonerror,
              dbadd,dbremove):
    rv = 1
    hashdb = {}
    for (filepath,fi) in db.items():
        if filefilter.filter(filepath,fi):
            continue

        if not fi.md5 in hashdb:
            hashdb[fi.md5] = []

        hashdb[fi.md5].append(filepath)

    actions = 0
    for (hashval,filepaths) in hashdb.items():
        if len(filepaths) > 1:
            continue

        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        print(hashval,shlex.quote(filepaths[0]))

    return rv


def list_missing(db,basepath,filefilter,sort,
                 maxactions,verbose,breakonerror,
                 dbadd,dbremove):
    rv = 1
    filepaths = get_files(basepath,filefilter)

    actions = 0
    output  = []
    for (filepath,fi) in db.items():
        if filepath in db:
            continue

        if actions >= maxactions:
            break
        actions += 1

        rv = 0
        output.append(filepath)

    sort(output)
    for filepath in output:
        print(filepath)

    return rv


def find(db,basepath,filefilter,sort,
         maxactions,verbose,breakonerror,
         dbadd,dbremove):
    rv = 1
    sizedb = set()
    hashdb = {}
    writer = csv.writer(sys.stdout,delimiter=',')
    for (filepath,fi) in db.items():
        if not fi.md5 in hashdb:
            hashdb[fi.md5] = []

        hashdb[fi.md5].append(filepath)
        sizedb.add(fi.size)

    actions = 0
    filepaths = get_files(basepath,filefilter)
    for (filepath,fi) in filepaths:
        try:
            if fi.size not in sizedb:
                if verbose > 1:
                    writer.writerow(('::NOTFOUND::',filepath))
                continue

            hashval = hash_file(filepath)
            if hashval not in hashdb:
                if verbose > 1:
                    writer.writerow(('::NOTFOUND::',filepath))
                continue

            if actions >= maxactions:
                return rv;
            actions += 1

            rv = 0
            if verbose:
                t = tuple([hashval,filepath] + hashdb[hashval])
                writer.writerow(t)
        except Exception as e:
            print("ERROR:",e)

    return rv


def is_not_sticky(fi):
    return not bool(fi.mode & stat.S_ISVTX)


def is_not_readonly(fi):
    return not (fi.mode & (stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH))


def restrict_fun(rtype):
    if rtype == 'sticky':
        return is_not_sticky
    elif rtype == 'readonly':
        return is_not_readonly
    return (lambda st: False)


def fnfilter_fun(regex):
    if regex:
        cregex = re.compile(regex)
        return (lambda filepath: cregex.match(filepath) == None)
    return (lambda filepath: False)


def inst_fun(inst):
    if inst == 'add':
        return add_hashes
    elif inst == 'append':
        return append_hashes
    elif inst == 'check':
        return check_hashes
    elif inst == 'check+update':
        return check_and_update_hashes
    elif inst == 'delete':
        return delete_hashes
    elif inst == 'cleanup':
        return cleanup_hashes
    elif inst == 'list':
        return list_hashes
    elif inst == 'list-unhashed':
        return list_unhashed
    elif inst == 'list-dups':
        return list_dups
    elif inst == 'list-solo':
        return list_solo
    elif inst == 'list-missing':
        return list_missing
    elif inst == 'find':
        return find
    return None


def sort_fun(sort):
    if sort == 'radix':
        return (lambda l: l.sort())
    elif sort == 'reverse-radix':
        return (lambda l: l.sort(reverse=True))
    elif sort == 'random':
        return (lambda l: random.shuffle(l))
    elif sort == 'natural':
        cre = re.compile('(\d+)')
        sort_key = lambda s: [int(t) if t.isdigit() else t.lower()
                              for t in re.split(cre,s[0])]
        return (lambda l: l.sort(key=sort_key))
    elif sort == 'reverse-natural':
        cre = re.compile('(\d+)')
        sort_key = lambda s: [int(t) if t.isdigit() else t.lower()
                              for t in re.split(cre,s[0])]
        return (lambda l: l.sort(key=sort_key,reverse=True))
    elif sort == 'time-asc':
        sort_key = lambda s: s[1].mtime
        return (lambda l: l.sort(key=sort_key))
    elif sort == 'time-desc':
        sort_key = lambda s: s[1].mtime
        return (lambda l: l.sort(key=sort_key,reverse=True))
    return (lambda l: None)


def read_pickle_db(filepath):
    db = {}
    try:
        with open(filepath,'rb') as f:
            db = pickle.load(f)
        print('converting hash database...')
        for (k,(h,st)) in db.items():
            db[k] = FileInfo(md5=h,
                             size=st.st_size,
                             mode=st.st_mode,
                             mtime=st.st_mtime)
        write_db_core(filepath,db)
    except:
        pass

    return db


def read_db(filepath):
    db = read_pickle_db(filepath)
    if db:
        return db
    try:
        with open(filepath,'rt',encoding='utf-8',newline='') as f:
            reader = csv.reader(f,delimiter=',',quotechar='"')
            for (filename,md5,size,mode,mtime) in reader:
                db[filename]=FileInfo(md5,
                                      int(size),
                                      int(mode),
                                      float(mtime))
    except (KeyboardInterrupt,SystemExit):
        raise
    except Exception as e:
        dirname = os.path.dirname(filepath)
        if not os.path.isdir(dirname):
            raise
    return db


def write_db_core(filepath,db):
    basepath = os.path.dirname(filepath)
    (fd,tmpfilepath) = tempfile.mkstemp(dir=basepath)
    with os.fdopen(fd,'wt',encoding='utf-8',newline='') as f:
        writer = csv.writer(f,delimiter=',')
        for (k,v) in db.items():
            row = (k,v.md5,v.size,v.mode,v.mtime)
            writer.writerow(row)
    os.replace(tmpfilepath,filepath)


def write_db(filepath,dbadd,dbremove):
    try:
        db = read_db(filepath)

        for (k,v) in dbadd.items():
            db[k] = v;
        for k in dbremove:
            del db[k]

        write_db_core(filepath,db)
    except (KeyboardInterrupt,SystemExit):
        raise
    except Exception as e:
        print('Error writing hash DB:',e)

def process_directories(dirs):
    rv = []
    for d in dirs:
        realpath = os.path.realpath(d)
        if realpath != '/':
            realpath + os.path.sep
        rv.append(realpath)
    return rv


def main():
    parser = build_arg_parser()
    args   = parser.parse_args()

    dbpath       = os.path.realpath(args.db)
    verbose      = args.verbose
    func         = inst_fun(args.inst)
    sort         = sort_fun(args.sort)
    fnfilter     = fnfilter_fun(args.fnfilter)
    fifilter     = restrict_fun(args.restrict)
    maxactions   = args.max
    breakonerror = args.break_on_error
    directories  = process_directories(args.dir)

    rv = 0
    try:
        for directory in directories:
            filefilter = FileFilter(basepath=directory,
                                    fnfilter=fnfilter,
                                    fifilter=fifilter)
            db = read_db(dbpath)
            dbadd = {}
            dbremove = []
            filefilter.basepath = directory

            rv = rv or func(db,directory,filefilter,sort,
                            maxactions,verbose,breakonerror,
                            dbadd,dbremove)

            if len(dbadd) or len(dbremove):
                write_db(dbpath,dbadd,dbremove)

            if breakonerror and rv:
                break

    except (KeyboardInterrupt,SystemExit):
        rv = 1
    except IOError as e:
        rv = 1
        if e.errno != errno.EPIPE:
            print(e)
    except Exception as e:
        rv = 1
        raise

    sys.exit(rv)


if __name__ == "__main__":
    main()
