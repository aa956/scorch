#!/usr/bin/env python3

# ISC License (ISC)
#
# Copyright (c) 2016, Antonio SJ Musumeci <trapexit@spawn.link>
#
# Permission to use, copy, modify, and/or distribute this software for any
# purpose with or without fee is hereby granted, provided that the above
# copyright notice and this permission notice appear in all copies.
#
# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

import argparse
import collections
import csv
import errno
import hashlib
import math
import os
import pickle
import random
import re
import shlex
import stat
import sys
import tempfile
import time


DEFAULT_DB = '/var/tmp/scorch.db'

class FileInfo(object):
    md5   = ''
    size  = 0
    mode  = 0
    mtime = 0

    def __init__(self,size,mode,mtime):
        self.size  = size
        self.mode  = mode
        self.mtime = mtime

    def __init__(self,md5,size,mode,mtime):
        self.md5   = md5
        self.size  = size
        self.mode  = mode
        self.mtime = mtime

    def __str__(self):
        return str({'md5': self.md5,
                    'size': self.size,
                    'mode': self.mode,
                    'mtime': self.mtime})


def build_arg_parser():
    desc = 'a tool to help discover file corruption'
    parser = argparse.ArgumentParser(description=desc)
    parser.add_argument('-d','--db',
                        type=str,
                        default=DEFAULT_DB,
                        help='database which stores hashes')
    parser.add_argument('inst',
                        choices=['add','append','check',
                                 'check+update','delete',
                                 'cleanup','list',
                                 'list-unhashed','list-dups',
                                 'list-missing','find'],
                        help='actions')
    parser.add_argument('dir',
                        type=str,
                        nargs='+',
                        help='directories to work on')
    parser.add_argument('-v','--verbose',
                        action='store_true',
                        help='print details of files')
    parser.add_argument('-r','--restrict',
                        choices=['sticky',
                                 'readonly'],
                        help='restrict action to certain types of files')
    parser.add_argument('-f','--fnfilter',
                        type=str,
                        help='restrict action to files which match regex')
    parser.add_argument('-s','--sort',
                        choices=['none','radix','reverse-radix',
                                 'natural','reverse-natural','random'],
                        help='when adding/appending/checking sort files before acting on them')
    parser.add_argument('-m','--max',
                        type=int,
                        default=sys.maxsize,
                        help='max number of actions to take')
    parser.add_argument('-b','--break-on-error',
                        action='store_true',
                        default=False,
                        help='break on first failure / error')

    return parser


def hash_file(filepath, hasher=None, blocksize=65536):
    if not hasher:
        hasher = hashlib.md5()

    with open(filepath,'rb') as afile:
        buf = afile.read(blocksize)
        while buf:
            hasher.update(buf)
            buf = afile.read(blocksize)

    return hasher.hexdigest()


def get_file_list(basepath,fnfilter,db=None):
    if os.path.isfile(basepath):
        return [basepath]

    filelist = []
    for (dirname,dirnames,filenames) in os.walk(basepath):
        fulldirpath = os.path.join(basepath,dirname)
        for filename in filenames:
            filepath = os.path.join(fulldirpath,filename)
            if fnfilter(filepath):
                continue
            if db and filepath in db:
                continue
            filelist.append(filepath)
    return filelist


def filter_filepaths(filepaths,basepath,fnfilter,other=(lambda f: False)):
    if basepath in filepaths:
        return [basepath]

    rv = []
    for filepath in filepaths:
        if not filepath.startswith(basepath):
            continue
        if fnfilter(filepath):
            continue
        if other(filepath):
            continue
        rv.append(filepath)

    return rv


def get_fileinfo(filepath):
    try:
        st = os.lstat(filepath)
        if not stat.S_ISREG(st.st_mode):
            return None
        return FileInfo(md5='',
                        size=st.st_size,
                        mode=st.st_mode,
                        mtime=st.st_mtime)
    except:
        return None


def print_filepath(count,total,filepath):
    padding = len(str(total))
    padded = str(count).zfill(padding)
    s = '{0}/{1} {2}: '.format(padded,total,filepath)
    print(s,end='')
    sys.stdout.flush()


def humansize(nbytes):
    suffixes = ['B','KB','MB','GB','TB','PB','ZB']
    rank = int(math.log(nbytes,1024))
    rank = min(rank, len(suffixes) - 1)
    human = nbytes / (1024.0 ** rank)
    f = ('%.2f' % human).rstrip('0').rstrip('.')
    return '%s%s' % (f, suffixes[rank])


# Can't use inode since filesystems based on FUSE can have those change
# mount to mount
def different_files(oldfi,newfi):
    return ((oldfi.size  != newfi.size) and
            (oldfi.mtime != newfi.mtime))


def add_hashes(db,basepath,restrict,fnfilter,sort,
               maxactions,verbose,breakonerror,
               dbadd,dbremove):
    rv = 1
    filepaths = get_file_list(basepath,fnfilter)
    sort(filepaths)

    actions = 0
    total = min(maxactions,len(filepaths))
    for filepath in filepaths:
        fi = get_fileinfo(filepath)
        if not fi:
            continue
        if not restrict(fi):
            continue

        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        if verbose:
            print_filepath(actions,total,filepath)

        try:
            fi.md5 = hash_file(filepath)
            dbadd[filepath] = fi
            if verbose:
                print(fi.md5)
        except Exception as e:
            if verbose:
                print("ERROR:",e)
            if breakonerror:
                break

    return rv


def append_hashes(db,basepath,restrict,fnfilter,sort,
                  maxactions,verbose,breakonerror,
                  dbadd,dbremove):
    rv = 1
    filepaths = get_file_list(basepath,fnfilter,db)
    sort(filepaths)

    actions = 0
    total = min(maxactions,len(filepaths))
    for filepath in filepaths:
        fi = get_fileinfo(filepath)
        if not fi:
            continue
        if not restrict(fi):
            continue

        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        if verbose:
            print_filepath(actions,total,filepath)

        try:
            fi.md5 = hash_file(filepath)
            dbadd[filepath] = fi
            if verbose:
                print(fi.md5)
        except (KeyboardInterrupt,SystemExit):
            raise
        except Exception as e:
            if verbose:
                print(e)
            if breakonerror:
                break

    return rv


def check_hashes(db,basepath,restrict,fnfilter,sort,
                 maxactions,verbose,breakonerror,
                 dbadd,dbremove,update=False):
    rv = 0
    filepaths = filter_filepaths(db.keys(),basepath,fnfilter)
    sort(filepaths)

    actions = 0
    total = min(maxactions,len(filepaths))
    for filepath in filepaths:
        fi = get_fileinfo(filepath)
        if not fi:
            continue
        if not restrict(fi):
            continue

        if actions >= maxactions:
            return rv
        actions += 1

        if verbose:
            print_filepath(actions,total,filepath)

        try:
            oldfi = db[filepath]
            if different_files(oldfi,fi):
                if not verbose:
                    print_filepath(actions,total,filepath)
                old_size = humansize(oldfi.size)
                new_size = humansize(fi.size)
                old_time = time.ctime(oldfi.mtime)
                new_time = time.ctime(fi.mtime)
                print("FILE CHANGED\n",
                      "- size:",old_size,"->",new_size,"\n"
                      " - mtime:",old_time,"->",new_time)
                if update:
                    fi.md5 = hash_file(filepath)
                    dbadd[filepath] = fi
                    print(" - updated hash:",fi.md5)
            else:
                oldhashval = db[filepath].md5
                newhashval = hash_file(filepath)
                if newhashval != oldhashval:
                    rv = 1
                    if not verbose:
                        print_filepath(actions,total,filepath)
                    print("FAILED")
                    if breakonerror:
                        break
                elif verbose:
                    print("OK")
        except (KeyboardInterrupt,SystemExit):
            raise
        except Exception as e:
            traceback.print_exc()
            if not verbose:
                print_filepath(actions,total,filepath)
            print('ERROR:',e)
            if breakonerror:
                break

    return rv


def check_and_update_hashes(db,basepath,restrict,fnfilter,sort,
                            maxactions,verbose,breakonerror,
                            dbadd,dbremove):
    return check_hashes(db,basepath,restrict,fnfilter,sort,
                        maxactions,verbose,breakonerror,
                        dbadd,dbremove,update=True)


def delete_hashes(db,basepath,restrict,fnfilter,sort,
                  maxactions,verbose,breakonerror,
                  dbadd,dbremove):
    rv = 1
    filepaths = filter_filepaths(db.keys(),basepath,fnfilter)
    sort(filepaths)

    actions = 0
    total = min(maxactions,len(filepaths))
    for filepath in filepaths:
        fi = get_fileinfo(filepath)
        if fi and not restrict(fi):
            continue

        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        dbremove.append(filepath)
        if verbose:
            print_filepath(actions,total,filepath)
            print("removed")

    return rv


def cleanup_hashes(db,basepath,restrict,fnfilter,sort,
                   maxactions,verbose,breakonerror,
                   dbadd,dbremove):
    rv = 1
    filepaths = filter_filepaths(db.keys(),basepath,fnfilter,os.path.exists)
    sort(filepaths)

    actions = 0
    total = min(maxactions,len(filepaths))
    for filepath in filepaths:
        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        dbremove.append(filepath)
        if verbose:
            print_filepath(actions,total,filepath)
            print("removed")

    return rv


def list_hashes(db,basepath,restrict,fnfilter,sort,
                maxactions,verbose,breakonerror,
                dbadd,dbremove):
    rv = 1
    filepaths = filter_filepaths(db.keys(),basepath,fnfilter)
    sort(filepaths)

    actions = 0
    for filepath in filepaths:
        fi = db[filepath]
        if not restrict(fi):
            continue

        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        print("{0} {1}".format(fi.md5,filepath))

    return rv


def list_unhashed(db,basepath,restrict,fnfilter,sort,
                  maxactions,verbose,breakonerror,
                  dbadd,dbremove):
    rv = 1
    filepaths = get_file_list(basepath,fnfilter,db)
    sort(filepaths)

    actions = 0
    for filepath in filepaths:
        fi = get_fileinfo(filepath)
        if not fi:
            continue
        if not restrict(fi):
            continue

        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        print(filepath)

    return rv


def list_dups(db,basepath,restrict,fnfilter,sort,
              maxactions,verbose,breakonerror,
              dbadd,dbremove):
    rv = 1
    hashdb = {}
    for (filepath,fi) in db.items():
        if not filepath.startswith(basepath):
            continue
        if fnfilter(filepath):
            continue
        if not restrict(fi):
            continue

        if not fi.md5 in hashdb:
            hashdb[fi.md5] = [filepath]
            continue

        hashdb[fi.md5].append(filepath)

    actions = 0
    for (hashval,filepaths) in hashdb.items():
        if len(filepaths) <= 1:
            continue

        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        filepaths = [shlex.quote(filepath) for filepath in filepaths]
        filepaths.sort()
        print(hashval,' '.join(filepaths))

    return rv


def list_missing(db,basepath,restrict,fnfilter,sort,
                 maxactions,verbose,breakonerror,
                 dbadd,dbremove):
    rv = 1
    filepaths = get_file_list(basepath,fnfilter)

    actions = 0
    output  = []
    for (filepath,fi) in db.items():
        if not filepath.startswith(basepath):
            continue
        if fnfilter(filepath):
            continue
        if filepath in filepaths:
            continue

        if actions >= maxactions:
            return rv
        actions += 1

        rv = 0
        output.append(filepath)

    sort(output)
    for filepath in output:
        print(filepath)

    return rv


def find(db,basepath,restrict,fnfilter,sort,
         maxactions,verbose,breakonerror,
         dbadd,dbremove):
    rv = 1
    hashdb = {}
    for (filepath,fi) in db.items():
        if not fi.md5 in hashdb:
            hashdb[fi.md5] = [filepath]
            continue
        hashdb[fi.md5].append(filepath)

    actions = 0
    filepaths = get_file_list(basepath,fnfilter)
    for filepath in filepaths:
        try:
            if actions >= maxactions:
                return rv;

            fi = get_fileinfo(filepath)
            if not fi:
                continue
            if not restrict(fi):
                continue

            hashval = hash_file(filepath)
            if hashval not in hashdb:
                continue

            rv = 0
            actions += 1

            files = ' '.join([shlex.quote(f) for f in hashdb[hashval]])
            quoted_filepath = shlex.quote(filepath)
            if verbose:
                print(hashval,quoted_filepath,files)
            else:
                print(quoted_filepath)
        except Exception as e:
            print("ERROR:",e)

    return rv


def is_sticky(fi):
    return bool(fi.mode & stat.S_ISVTX)


def is_readonly(st):
    return not (fi.mode & (stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH))


def restrict_fun(rtype):
    if rtype == 'sticky':
        return is_sticky
    elif rtype == 'readonly':
        return is_readonly
    return (lambda st: True)


def filter_fun(regex):
    if regex:
        cregex = re.compile(regex)
        return (lambda filepath: cregex.match(filepath) == None)
    return (lambda filepath: False)


def inst_fun(inst):
    if inst == 'add':
        return add_hashes
    elif inst == 'append':
        return append_hashes
    elif inst == 'check':
        return check_hashes
    elif inst == 'check+update':
        return check_and_update_hashes
    elif inst == 'delete':
        return delete_hashes
    elif inst == 'cleanup':
        return cleanup_hashes
    elif inst == 'list':
        return list_hashes
    elif inst == 'list-unhashed':
        return list_unhashed
    elif inst == 'list-dups':
        return list_dups
    elif inst == 'list-missing':
        return list_missing
    elif inst == 'find':
        return find
    return None


def sort_fun(sort):
    if sort == 'radix':
        return (lambda l: l.sort())
    elif sort == 'reverse-radix':
        return (lambda l: l.sort(reverse=True))
    elif sort == 'random':
        return (lambda l: random.shuffle(l))
    elif sort == 'natural':
        cre = re.compile('(\d+)')
        sort_key = lambda s: [int(t) if t.isdigit() else t.lower()
                              for t in re.split(cre,s)]
        return (lambda l: l.sort(key=sort_key))
    elif sort == 'reverse-natural':
        cre = re.compile('(\d+)')
        sort_key = lambda s: [int(t) if t.isdigit() else t.lower()
                              for t in re.split(cre,s)]
        return (lambda l: l.sort(key=sort_key,reverse=True))
    return (lambda l: None)


def read_pickle_db(filepath):
    db = {}
    try:
        with open(filepath,'rb') as f:
            db = pickle.load(f)
        print('converting hash database...')
        for (k,(h,st)) in db.items():
            db[k] = FileInfo(md5=h,
                             size=st.st_size,
                             mode=st.st_mode,
                             mtime=st.st_mtime)
        write_db_core(filepath,db)
    except:
        pass

    return db


def read_db(filepath):
    db = read_pickle_db(filepath)
    if db:
        return db
    try:
        with open(filepath,'rt',encoding='utf-8',newline='') as f:
            reader = csv.reader(f,delimiter=',',quotechar='"')
            for (filename,md5,size,mode,mtime) in reader:
                db[filename]=FileInfo(md5,
                                      int(size),
                                      int(mode),
                                      float(mtime))
    except (KeyboardInterrupt,SystemExit):
        raise
    except Exception as e:
        dirname = os.path.dirname(filepath)
        if not os.path.isdir(dirname):
            raise
    return db


def write_db_core(filepath,db):
    basepath = os.path.dirname(filepath)
    (fd,tmpfilepath) = tempfile.mkstemp(dir=basepath)
    with os.fdopen(fd,'wt',encoding='utf-8',newline='') as f:
        writer = csv.writer(f,delimiter=',')
        for (k,v) in db.items():
            row = (k,v.md5,v.size,v.mode,v.mtime)
            writer.writerow(row)
    os.replace(tmpfilepath,filepath)


def write_db(filepath,dbadd,dbremove):
    try:
        db = read_db(filepath)

        for (k,v) in dbadd.items():
            db[k] = v;
        for k in dbremove:
            del db[k]

        write_db_core(filepath,db)
    except (KeyboardInterrupt,SystemExit):
        raise
    except Exception as e:
        print('Error writing hash DB:',e)

def process_directories(dirs):
    rv = []
    for d in dirs:
        realpath = os.path.realpath(d)
        if realpath != '/':
            realpath + os.path.sep
        rv.append(realpath)
    return rv


def main():
    parser = build_arg_parser()
    args   = parser.parse_args()

    dbpath       = os.path.realpath(args.db)
    verbose      = args.verbose
    restrict     = restrict_fun(args.restrict)
    func         = inst_fun(args.inst)
    sort         = sort_fun(args.sort)
    fnfilter     = filter_fun(args.fnfilter)
    maxactions   = args.max
    breakonerror = args.break_on_error
    directories  = process_directories(args.dir)

    rv = 0
    try:
        for directory in directories:
            db = read_db(dbpath)
            dbadd = {}
            dbremove = []

            rv = rv or func(db,directory,restrict,fnfilter,sort,
                            maxactions,verbose,breakonerror,
                            dbadd,dbremove)

            if len(dbadd) or len(dbremove):
                write_db(dbpath,dbadd,dbremove)

            if breakonerror and rv:
                break

    except (KeyboardInterrupt,SystemExit):
        rv = 1
    except IOError as e:
        rv = 1
        if e.errno != errno.EPIPE:
            print(e)
    except Exception as e:
        rv = 1
        print(e)

    sys.exit(rv)


if __name__ == "__main__":
    main()
